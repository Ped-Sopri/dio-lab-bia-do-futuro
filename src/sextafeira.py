# ============================================================
# CHATBOT "SEXTA-FEIRA" (Streamlit + Ollama + √Åudio + PostgreSQL)
# ============================================================
# ESTE ARQUIVO √â UM APP STREAMLIT. Para conclus√£o, do curso da DIO+Bradesco.
# Principal objetivo para mim neste projeto √© tentar criar algo que juntasse todas as t√©cnologias que aprendi no curso.
# Que seria a integra√ßao de um banco de dados IA e programa√ß√£o. 
# Observa√ß√µes: 
# TTL demonstrativo para fins educacionais. Em produ√ß√£o, o valor deve variar conforme a natureza dos dados: - dados est√°ticos ‚Üí TTL alto - dados din√¢micos ‚Üí TTL baixo
# Implementei audio conforme desenvolvido durante aula em curso, mas sabemos que ja existe tipos mais realistas como ja sitado no curso.
# Foi escolhido o modelo gpt-oss:120b-cloud do Ollama por ser gratuito, estar na nuvem e ter um n√∫mero de par√¢metros muito alto, o que pode proporcionar respostas mais ricas e detalhadas para as intera√ß√µes do chatbot, especialmente considerando a complexidade do tema de investimentos financeiros.
# Foi escolhido o PostgreSQL para o banco de dados por ser um sistema de gerenciamento de banco de dados relacional robusto, amplamente utilizado e ser gratuito.
# Fui adepto ao Streamlit por sua facilidade para cria√ß√£o de interface.
# Criptografia de senhas com hash e SALT para seguran√ßa dos dados dos usu√°rios n√£o era o foco do projeto, mas quis implementar para demonstrar o conceito de seguran√ßa de senhas e boas pr√°ticas no armazenamento de informa√ß√µes sens√≠veis. 
# ============================================================
#=========================BIBLIOTECAS=========================
import hashlib          #importa a biblioteca hashlib para fun√ß√µes de hash (seguran√ßa de senhas,criptografia)
import streamlit as st  #importa a biblioteca Streamlit para criar a interface
import pandas as pd     #importa a biblioteca Pandas para manipula√ß√£o de dados(Tabelas, DataFrames)
import requests         #importa a biblioteca Requests para fazer requisi√ß√µes HTTP (comunica√ß√£o com Ollama)
import speech_recognition as sr    #importa a biblioteca SpeechRecognition para transcri√ß√£o de √°udio (TTS)
import tempfile             #importa a biblioteca tempfile para criar arquivos tempor√°rios (armazenar √°udio recebido do usuario)
from gtts import gTTS       #importa a biblioteca gTTS (Google Text-to-Speech) para converter texto em √°udio (respostas faladas da sexta-feira)
import io          #importa a biblioteca io para manipula√ß√£o de fluxos de dados em mem√≥ria (gerar √°udio em bytes para o Streamlit)
import os           #importa a biblioteca os para opera√ß√µes do sistema (ex: remover arquivos tempor√°rios de √°udio)
import re           #importa a biblioteca re para express√µes regulares (valida√ß√£o e formata√ß√£o de CPF, limpeza de texto para TTS)
from sqlalchemy import create_engine, text        #importa fun√ß√µes do SQLAlchemy para conex√£o e execu√ß√£o de consultas no banco PostgreSQL (armazenamento de dados dos usu√°rios, produtos, hist√≥rico)
#===============================================================
#comando em streamlit para configurar titulo(Chatbot-Investimentos) da pagina icone (ü§ñ) e como layout vai ser wide que √© para ocupar toda a largura da tela.
st.set_page_config(page_title="Chatbot-Investimentos", page_icon="ü§ñ", layout="wide") 
#===============================================================
#=========================CONEX√ÉO COM POSTGRES=========================
@st.cache_resource(show_spinner=False) # uma das dificuldades ao utilizar os bancos de dados foi melhorar a velocidade e evitar conex√µes repetidas, com esse comando do Streamlit a fun√ß√£o get_engine s√≥ √© executada uma vez e o resultado √© armazenado em cache, ou seja, a conex√£o com o banco √© criada apenas na primeira vez que a fun√ß√£o √© chamada e reutilizada nas chamadas seguintes, melhorando a performance do app.
def get_engine():       #cria√ß√£o de uma fun√ß√£o para estabelecer a conex√£o com o banco de dados PostgreSQL usando SQLAlchemy, utilizando as credenciais armazenadas em st.secrets para seguran√ßa.
    host = "localhost"  # endere√ßo do servidor do banco de dados 
    port = 5432         # porta padr√£o do PostgreSQL
    dbname = "sextafeira"       # nome do banco de dados a ser acessado
    user = "postgres"           # nome de usu√°rio para autentica√ß√£o no banco de dados
    password = st.secrets["DB_PASSWORD"]        # senha para autentica√ß√£o, armazenada de forma segura em st.secrets.

    url = f"postgresql+psycopg://{user}:{password}@{host}:{port}/{dbname}" # URL de conex√£o formatada para SQLAlchemy, especificando o driver (psycopg) e as credenciais necess√°rias para acessar o banco de dados PostgreSQL escritas acima.
    return create_engine(url, pool_pre_ping=True)       #retorna uma engine de conex√£o com a url especificada. O par√¢metro pool_pre_ping=True √© usado para garantir que a conex√£o seja verificada antes de ser reutilizada, evitando erros de conex√£o ociosas.
                                                        #pool_pre _ping "pinga" na url e a engine √© a ponte que conecta este c√≥digo ao banco de dados.
#=============================================================
#=========================CONFIG. SENHA=========================
def cpf_para_digitos(cpf_formatado: str) -> str:  #cria√ß√£o de uma fun√ß√£o para converter um CPF formatado (com pontos e tra√ßo) em apenas d√≠gitos, utilizando express√µes regulares para remover qualquer caractere que n√£o seja n√∫mero, garantindo que o CPF seja processado corretamente para consultas no banco de dados.
    return re.sub(r"\D", "", cpf_formatado or "")

def cpf_formatado_ok(cpf: str) -> bool:         #cria√ß√£o de uma fun√ß√£o para validar se um CPF est√° no formato correto (000.000.000-00) usando express√µes regulares, garantindo que os usu√°rios insiram o CPF de forma adequada antes de tentar fazer login ou acessar seus dados.
    return bool(re.fullmatch(r"\d{3}\.\d{3}\.\d{3}-\d{2}", cpf or ""))

SALT = st.secrets["SALT_VAL"]  # variavel utilizada para "misturar" com a senha real do usuario criando o valor secreto para o hash

def hash_senha(senha: str) -> str:    #fun√ß√£o para criar um hash seguro da senha do usu√°rio, combinando a senha com um valor de SALT, converte para bytes com UTF-8 e utilizando o algoritmo SHA-256 para gerar um hash hexadecimal, protegendo as informa√ß√µes dos usu√°rios.
    return hashlib.sha256((senha + SALT).encode("utf-8")).hexdigest()  #poderia ter utilizado outros como brcrypt ou argon2 que s√£o mais robustos para senhas, mas para este projeto e com o uso do SALT, o SHA-256 √© suficiente para demonstrar o conceito de hashing de senhas.

def buscar_senha_hash_banco(cpf_digits: str) -> str | None: #fun√ß√£o para buscar o hash da senha do usu√°rio no banco de dados, utilizando a conex√£o estabelecida pela fun√ß√£o get_engine, executando uma consulta SQL que seleciona o campo senha_hash da tabela perfil_investidor onde o CPF (apenas d√≠gitos) corresponde ao CPF fornecido, e retornando o hash encontrado ou None se o CPF n√£o for encontrado ou se o campo senha_hash for nulo.
    engine = get_engine()       #obt√©m a engine de conex√£o com o banco de dados usando a fun√ß√£o get_engine definida anteriormente.

    with engine.connect() as conn:  # com a conex√£o SQL estabelecida ececuta um comando sql para encontrar o cpf do usu√°rio e retornar o hash da senha correspondente, utilizando uma express√£o regular para garantir que o CPF seja comparado apenas pelos d√≠gitos.
        result = conn.execute(
            text("""
                SELECT senha_hash
                FROM perfil_investidor
                WHERE regexp_replace(cpf::text, '[^0-9]', '', 'g') = :cpf
                LIMIT 1
            """),
            {"cpf": cpf_digits}
        )

        row = result.fetchone()         #fetchone() √© usado para obter a primeira linha do resultado da consulta, que deve conter o hash da senha se o CPF for encontrado. 

    if not row or row[0] is None: # se n√£o houver resultado ou se o campo senha_hash for nulo, a fun√ß√£o retorna None, indicando que o CPF n√£o foi encontrado.
        return None

    return str(row[0])  # se o hash da senha for encontrado, ele √© convertido para string e retornado para ser comparado com o hash da senha fornecida pelo usu√°rio durante o login.

# ====================================================================
# =====================PAGINA DE ACESSO=========================
st.session_state.setdefault("logado", False)   #inicializa a vari√°vel de estado "logado" como False, indicando que o usu√°rio n√£o est√° logado por padr√£o. O setdefault √© usado para garantir que essa vari√°vel exista no session_state, evitando erros caso seja acessada antes de ser definida.
st.session_state.setdefault("visitante", False)  #inicializa a vari√°vel de estado "visitante" como False, indicando que o usu√°rio n√£o escolheu continuar como visitante por padr√£o. Assim como "logado", o setdefault garante que essa vari√°vel exista no session_state para evitar erros.
st.session_state.setdefault("cpf_formatado", "") #inicializa a vari√°vel de estado "cpf_formatado" como uma string vazia, que ser√° usada para armazenar o CPF formatado do usu√°rio quando ele fizer login. O setdefault garante que essa vari√°vel exista no session_state, permitindo que seja atualizada posteriormente com o CPF do usu√°rio logado.
if not st.session_state.logado and not st.session_state.visitante:  #condi√ß√£o para que caso n√£o esteja logado permane√ßa nesta pagina de acesso, onde o usu√°rio pode escolher entre fazer login ou continuar como visitante. Se nenhuma das condi√ß√µes for verdadeira (ou seja, o usu√°rio n√£o est√° logado e n√£o escolheu ser visitante), o c√≥digo dentro deste bloco ser√° executado, exibindo a interface de login e op√ß√µes para o usu√°rio.
    st.title("üîê Acesso ao Sistema")        #titulo da pagina
    st.warning("Fa√ßa login ou continue como visitante.")        #aviso para usu√°rio.
    st.warning("Usu√°rios n√£o identificados ter√£o acesso limitado a recomenda√ß√µes gen√©ricas.") #aviso para usu√°rio.

    cpf = st.text_input("CPF", placeholder="000.000.000-00") #campo para usu√°rio digitar cpf, com placeholder para indicar o formato esperado.
    senha = st.text_input("Senha", type="password") #campo para usu√°rio digitar senha, com type="password" para ocultar os caracteres digitados.

    col1, col2 = st.columns(2)  #divis√£o da interface abaixo co campo senha em duas colunas.

    # BOT√ÉO LOGIN
    with col1:   #na coluna 1
        if st.button("Entrar"):  # cria√ß√£o do bot√£o entrar e condi√ß√£o para processar o login quando o bot√£o for clicado.
            if not cpf_formatado_ok(cpf):       #condi√ß√£o para que cpf esteja no formato certo se n√£o estiver  aparece a mensagem de st.error.
                st.error("Digite o CPF no formato 000.000.000-00.")
                st.stop()             #st.stop() √© usado para interromper a execu√ß√£o do c√≥digo ap√≥s exibir a mensagem de erro, garantindo que o restante do c√≥digo de login n√£o seja executado se o CPF estiver no formato incorreto.

            cpf_digits = cpf_para_digitos(cpf)  # vari√°vel cpf_digits recebe o resultado da fun√ß√£o cpf_para_digitos, que converte o CPF formatado em apenas d√≠gitos, removendo pontos e tra√ßos.
            if len(cpf_digits) != 11:      #condi√ß√£o para verificar se o CPF convertido tem exatamente 11 d√≠gitos, que √© o formato correto para CPFs no Brasil. Se n√£o tiver 11 d√≠gitos, exibe uma mensagem de erro indicando que o CPF √© inv√°lido e interrompe a execu√ß√£o do c√≥digo de login.
                st.error("CPF inv√°lido.")  #exibe mensagem de erro para CPF inv√°lido
                st.stop()     #interrompe a execu√ß√£o do c√≥digo para evitar que o login continue com um CPF inv√°lido.

            senha_hash_db = buscar_senha_hash_banco(cpf_digits)   #a variavel senha_hash_db recebe o resultado da fun√ß√£o buscar_senha_hash_banco, que consulta o banco de dados para obter o hash da senha correspondente ao CPF fornecido. Se o CPF n√£o for encontrado ou se o campo senha_hash for nulo, a fun√ß√£o retornar√° None.

            if senha_hash_db is None:  # condi√ß√£o para caso senha for none, exibe a mensagem de erro, e interrompe a execu√ß√£o do c√≥digo de login.
                st.error("CPF n√£o encontrado ou usu√°rio sem senha cadastrada.")
                st.stop()

            if hash_senha(senha) != senha_hash_db:   #condi√ß√£o para caso senha for diferente, exibe a mensagem de erro, e interrompe a execu√ß√£o do c√≥digo de login.
                st.error("Senha incorreta.")
                st.stop()

            st.session_state.logado = True  #se o login for bem-sucedido, a vari√°vel muda para True, fazendo com que usu√°rio mude de interface.
            st.session_state.cpf_formatado = cpf  #armazena o CPF formatado no session_state para uso posterior, como carregar os dados do usu√°rio e personalizar as recomenda√ß√µes de investimento.
            st.rerun()  #recarrega a p√°gina para refletir o estado de login atualizado, permitindo que o usu√°rio acesse a interface principal do chatbot ap√≥s um login bem-sucedido.

    with col2:  #na coluna 2
        if st.button("Continuar sem identifica√ß√£o"): #condi√ß√£o e cria√ß√£o do bot√£o como visitante, caso o usu√°rio clique neste bot√£o, a vari√°vel de estado "visitante" √© definida como True, indicando que o usu√°rio escolheu continuar sem se identificar. Em seguida, a p√°gina √© recarregada para refletir essa escolha e permitir que o usu√°rio acesse a interface do chatbot com acesso limitado a recomenda√ß√µes gen√©ricas.
            st.session_state.visitante = True  #vari√°vel muda para True, fazendo com que usu√°rio mude de interface.
            st.rerun()  #recarrega a p√°gina para refletir o estado de login atualizado, permitindo que o usu√°rio acesse a interface principal do chatbot ap√≥s um login bem-sucedido.
    st.stop()  # interrompe a execu√ß√£o do c√≥digo para garantir que o restante do app n√£o seja carregado enquanto o usu√°rio estiver na p√°gina de acesso, evitando que ele veja ou interaja com a interface principal do chatbot antes de fazer login ou escolher continuar como visitante.
#=============================================================
#=========================CONFIG. Ollama====================
Ollama_BASE = "http://localhost:11434"   #variavel √© a URL base para acessar a API do OLLAMA.
OLLAMA_CHAT_URL = f"{Ollama_BASE}/api/chat"  #variavel especifica o endpoint para enviar mensagens de chat e receber respostas do modelo, utilizado na fun√ß√£o ollama_chat para comunica√ß√£o com o modelo de linguagem.
OLLAMA_GEN_URL = f"{Ollama_BASE}/api/generate"  #variavel especifica o endpoint para enviar prompts de gera√ß√£o de texto e receber respostas do modelo, que poderia ser utilizado para tarefas de gera√ß√£o de texto mais simples ou espec√≠ficas.

DEFAULT_MODEL = "gpt-oss:120b-cloud"  #variavel define o modelo padr√£o a ser utilizado nas chamadas para o Ollama, que √© um modelo de linguagem grande (120 bilh√µes de par√¢metros) otimizado para execu√ß√£o em nuvem, oferecendo respostas mais r√°pidas e eficientes para as intera√ß√µes do chatbot.
# foi preferivel utilizar isto pois agrega o melhor dos dois mundos √© gratuito e esta na nuvem sem problema de lentid√£o e alem disso com 120B par√¢metros = absurdamente pesado.
REQUEST_TIMEOUT = 180 # Tempo m√°ximo (segundos) esperando resposta do Ollama.

def ler_df(sql_query: str, params=None) -> pd.DataFrame: #fun√ß√£o para ler dados do banco de dados e retornar um DataFrame do Pandas.
    engine = get_engine()       #obt√©m a engine de conex√£o com o banco de dados usando a fun√ß√£o get_engine definida anteriormente.
    return pd.read_sql_query(sql_query, engine, params=params)      #executa a consulta SQL fornecida na vari√°vel sql_query usando a conex√£o estabelecida pela engine, e retorna os resultados como um DataFrame do Pandas. O par√¢metro params √© opcional e pode ser usado para passar par√¢metros para consultas parametrizadas, ajudando a evitar inje√ß√£o de SQL e melhorar a seguran√ßa das consultas.
#definindo o que a IA √© e suas regras de funcionalidade.
SYSTEM_PROMPT_DEFAULT = """Voc√™ √© Sexta-Feira, uma intelig√™ncia artificial especializada em recomendar investimentos aos clientes de acordo com suas necessidades.

REGRAS:
1. Sempre baseie suas respostas nos dados fornecidos.
2. Nunca invente informa√ß√µes.
3. Respostas sempre concisas, objetivas, educadas e didaticas.
4. Se voc√™ n√£o souber algo, admita e ofere√ßa ao usuario se consultar com profissional certificado.
5. Se o cliente disser que quer ou est√° pensando em investir e n√£o especificou sempre ofere√ßa produtos de acordo com o seu perfil de investimento por√©m se for um usuario sem identifica√ß√£o pergunte, caso o usuario n√£o tenha especificado:
- Qual o n√≠vel de risco que voc√™ est√° disposto a assumir nesta transa√ß√£o?
- Qual o valor que voc√™ est√° disposto a investir?
6. Apresente as op√ß√µes de investimento especificando: nome, categoria, investimento m√≠nimo, risco e retorno.
7. Sempre informe se o investimento exigir√° o pagamento de imposto de renda ou n√£o.
8. Jamais responda qualquer pergunta al√©m do tema de investimentos financeiros e lembre ao usu√°rio que seu papel n√£o √© esse.
9. Se o usuario fizer uma pergunta se a a√ß√£o, moeda ou qualquer um ira aumentar ou diminuir o valor ou a cota√ß√£o diga que o melhor a se fazer √© consultar um profissional certificado, pois o mercado financeiro √© muito vol√°til e n√£o se pode confiar totalmente em previs√µes.
10. Se o cliente fizer perguntas sobre o mercado financeiro, investimentos ou produtos de investimento, responda com base nos dados fornecidos e informe que o mercado √© vol√°til e que √© importante consultar um profissional certificado para decis√µes de investimento.
11. Nunca divulgue informa√ß√µes pessoais do cliente.
12. Sempre que poss√≠vel, eduque o cliente sobre conceitos financeiros relacionados √† pergunta ou recomenda√ß√£o.
13. Exemplo de fala de confirma√ß√£o:"Sextou com sucesso!...(restante do texto).Exemplo de fala de erro ou limita√ß√£o:Infelizmente n√£o Sextou...(restante do texto).
"""
st.session_state.setdefault("mensagens", [{"role": "assistant", "content": "Sextou! Como posso ajudar?"}]) # mensagens do chat s√£o armazenadas no session_state do Streamlit, permitindo que o hist√≥rico de mensagens seja mantido durante a intera√ß√£o do usu√°rio com o chatbot. A mensagem inicial √© definida como "Sextou! Como posso ajudar?", indicando que o chatbot est√° pronto para receber perguntas e fornecer recomenda√ß√µes de investimento.
#=============================================================
#=========================MEMORIA DA IA=========================
st.session_state.setdefault("summary", "")  # resumo acumulado do chat
def obter_historico_curto(max_msgs=8):        #fun√ß√£o para obter o hist√≥rico de mensagens curto para enviar ao Ollama.
    historico = st.session_state.get("mensagens", []) #declara a variavel historico que recebe as mensagens do session_state, ou uma lista vazia caso n√£o haja mensagens. O session_state √© onde o Streamlit armazena o estado da aplica√ß√£o, e "mensagens" √© a chave onde o hist√≥rico de mensagens do chat √© mantido.
    return historico[-max_msgs:]                #retorna apenas as √∫ltimas max_msgs mensagens do hist√≥rico, o que pode melhorar a qualidade das respostas geradas pelo modelo.

def mensagem_para_texto(mensagens: list[dict]) -> str: #fun√ß√£o para converter uma lista de mensagens (dicion√°rios com "role" e "content") em um texto formatado, onde cada mensagem √© representada por seu papel (USER ou ASSISTANT) seguido do conte√∫do da mensagem, separados por quebras de linha. Isso √© √∫til para criar um bloco de texto que pode ser enviado ao modelo de linguagem para resumir ou analisar o hist√≥rico da conversa.
    linhas = []     #lista para armazenar as linhas formatadas do hist√≥rico de mensagens.
    for m in mensagens:    #itera sobre cada m = mensagem na lista de mensagens, onde cada mensagem √© um dicion√°rio que deve conter as chaves "role" e "content".
        role = m.get("role", "")        #obt√©m o valor da chave "role" da mensagem, que indica se a mensagem foi enviada pelo usu√°rio ("user") ou pela IA. Se a chave "role" n√£o estiver presente, retorna uma string vazia por padr√£o.
        content = (m.get("content") or "").strip()  #obtem o valor da mensagem em si. se a chave "content" n√£o estiver presente ou for None, retorna uma string vazia. O m√©todo strip() √© usado para remover espa√ßos em branco no in√≠cio e no final do conte√∫do da mensagem, garantindo que o texto seja limpo antes de ser adicionado √† lista de linhas.
        if role in ("user", "assistant") and content:   #verifica se o papel da mensagem √© "user" ou "assistant"(IA) e se o conte√∫do n√£o est√° vazio. Isso garante que apenas mensagens relevantes sejam inclu√≠das no hist√≥rico formatado.
            linhas.append(f"{role.upper()}: {content}")  #adiciona uma linha formatada a lista de linhas, onde o papel da mensagem √© convertido para letra mai√∫scula.
    return "\n".join(linhas) #retorna o hist√≥rico de mensagens formatado como um √∫nico bloco de texto, onde cada mensagem √© separada por uma quebra de linha. Isso facilita o envio do hist√≥rico para o modelo de linguagem para an√°lise ou resumo.

def ollama_resumir(modelo: str, resumo_atual: str, bloco: str) -> str:  #fun√ß√£o para resumir o hist√≥rico de mensagens usando o modelo do Ollama, onde resumo_atual √© o resumo acumulado at√© agora e bloco √© o novo texto formatado do hist√≥rico de mensagens que se deseja incorporar ao resumo. A fun√ß√£o constr√≥i um prompt para o modelo de linguagem, solicitando que ele atualize o resumo com base nas novas mensagens, mantendo as prefer√™ncias do usu√°rio, decis√µes e recomenda√ß√µes anteriores, e garantindo que o resultado seja conciso e em portugu√™s.
    prompt_resumo = f"""
        Voc√™ √© um sistema que resume conversas.
        RESUMO ATUAL (se existir):
        {resumo_atual}
        NOVAS MENSAGENS PARA INCORPORAR AO RESUMO:
        {bloco}
        Tarefa:
        - Atualize o resumo incorporando as novas mensagens.
        - Mantenha em portugu√™s.
        - Seja curto (m√°ximo ~12 linhas).
        - Preserve prefer√™ncias do usu√°rio (risco, valor, objetivos), decis√µes j√° tomadas e recomenda√ß√µes feitas.
        - N√ÉO inclua dados sens√≠veis.
        - N√ÉO invente nada.
        - Produza apenas o RESUMO atualizado.
        """.strip() #prompt para o modelo de linguagem, explicando como resumir e o . strip() √© usado para remover espa√ßos em branco no in√≠cio e no final do prompt, garantindo que o texto seja limpo antes de ser enviado ao modelo.
    payload = {  #este payload √© o corpo da requisi√ß√£o que ser√° enviada para o endpoint de chat do Ollama, contendo o modelo a ser utilizado, as mensagens (neste caso, apenas o prompt de resumo).
        "model": modelo,
        "messages": [{"role": "system", "content": prompt_resumo}],
        "options": { #op√ß√µes explicada mais abaixo na fun√ß√£o ollama_chat.
            "temperature": 0.1,
            "top_p": 0.7,
            "num_predict": 250,
            "repeat_penalty": 1.1,
        },
        "stream": False,
        }
    r = requests.post(OLLAMA_CHAT_URL, json=payload, timeout=REQUEST_TIMEOUT) #declara a variavel r de requisi√ß√£o que recebe a resposta da requisi√ß√£o POST feita para o endpoint de chat do Ollama, onde o payload √© enviado como JSON e o tempo m√°ximo de espera pela resposta √© definido por REQUEST_TIMEOUT. Esta requisi√ß√£o √© respons√°vel por enviar o prompt de resumo para o modelo de linguagem e obter a resposta que cont√©m o resumo atualizado. 
    if r.status_code != 200:  #condi√ß√£o caso r tenha um status code diferente de 200, o que indica que houve um erro na requisi√ß√£o (como modelo n√£o encontrado, problemas de conex√£o, etc). Se isso acontecer, a fun√ß√£o levanta uma exce√ß√£o RuntimeError com uma mensagem que inclui o status code e o texto da resposta, ajudando a identificar o motivo do erro.
        raise RuntimeError(f"Ollama resumo HTTP {r.status_code}: {r.text}") #levanta uma excess√£o de run time error com uma mensagem que inclui o status code e o texto da resposta, ajudando a identificar o motivo do erro caso a requisi√ß√£o para o Ollama n√£o seja bem-sucedida.
    return r.json()["message"]["content"].strip() #se der certo retorna o conteudo da mensagem da resposta do modelo que tem o resumo atual.

def talvez_resumir_chat(modelo: str, limite_total: int = 19, manter_ultimas: int = 8): #fun√ß√£o para verificar se o n√∫mero total de mensagens no hist√≥rico do chat ultrapassa um limite definido (limite_total). Se ultrapassar, a fun√ß√£o pega as mensagens mais antigas (tudo menos as √∫ltimas manter_ultimas) e as converte em um bloco de texto formatado usando a fun√ß√£o mensagem_para_texto. Em seguida, esse bloco √© enviado para a fun√ß√£o ollama_resumir, que atualiza o resumo acumulado com base nas novas mensagens. Por fim, a fun√ß√£o mant√©m apenas as √∫ltimas manter_ultimas mensagens no hist√≥rico do chat, garantindo que o hist√≥rico seja gerenciado de forma eficiente e que o modelo de linguagem tenha um contexto relevante para gerar respostas.
    msgs = st.session_state.get("mensagens", []) #declara a variavel msgs que recebe as mensagens do session_state, ou uma lista vazia caso n√£o haja mensagens. O session_state √© onde o Streamlit armazena o estado da aplica√ß√£o, e "mensagens" √© a chave onde o hist√≥rico de mensagens do chat √© mantido.
    # s√≥ resumimos se passou do limite
    if len(msgs) <= limite_total:  #condi√ß√£o para verificar se o n√∫mero total de mensagens no hist√≥rico do chat √© menor ou igual ao limite_total definido. Se for, a fun√ß√£o retorna imediatamente, sem fazer nada, pois n√£o √© necess√°rio resumir o hist√≥rico. Isso ajuda a evitar chamadas desnecess√°rias ao modelo de linguagem para resumir quando o n√∫mero de mensagens ainda est√° dentro do limite aceit√°vel.
        return  #se o n√∫mero de mensagens for menor ou igual aolimite, a fun√ß√£o sai.
    antigas = msgs[:-manter_ultimas]  #declara a variavel antigas que recebe as mensagens mais antigas do hist√≥rico, ou seja, todas as mensagens exceto as √∫ltimas manter_ultimas. Essas mensagens s√£o as que ser√£o resumidas para atualizar o resumo acumulado, enquanto as √∫ltimas manter_ultimas mensagens ser√£o mantidas no hist√≥rico para fornecer contexto recente ao modelo de linguagem.
    bloco = mensagem_para_texto(antigas).strip() #declara a variavel bloco que recebe o resultado da fun√ß√£o mensagem_para_texto aplicada √†s mensagens antigas, convertendo-as em um texto formatado. O m√©todo strip() √© usado para remover espa√ßos em branco no in√≠cio e no final do bloco de texto, garantindo que o texto seja limpo antes de ser enviado para a fun√ß√£o de resumo.
    if not bloco:  #se n√£o houver bloco de texto
        st.session_state["mensagens"] = msgs[-manter_ultimas:]  #mantem apenas as ultimas mensagens do hist√≥rico.
        return #retorna para evitar a fun√ß√£o de resumo caso o bloco esteja vazio.
    resumo_atual = st.session_state.get("summary", "")  # declara a variavel resumo_atual que recebe o resumo acumulado atual do session_state, ou uma string vazia caso n√£o haja resumo. O resumo acumulado √© o resultado das chamadas anteriores √† fun√ß√£o de resumo, e √© atualizado a cada vez que o hist√≥rico de mensagens ultrapassa o limite definido, permitindo que o modelo de linguagem mantenha um contexto relevante da conversa ao longo do tempo.
    try:
        novo_resumo = ollama_resumir(modelo, resumo_atual, bloco) #tenta chamar a fun√ß√£o ollama resumir para obter o resumo atualizado. 
    except Exception: #se ocorrer qualquer exce√ß√£o durante a chamada para o modelo de resumo, a fun√ß√£o captura a exce√ß√£o e simplesmente retorna, mantendo o resumo antigo e evitando que o hist√≥rico de mensagens seja perdido. Isso garante que, mesmo em caso de falhas na comunica√ß√£o com o modelo de linguagem, o chatbot continue funcionando sem perder o contexto acumulado.
        return # se falhou, mant√©m o resumo antigo (n√£o perde o que j√° tinha)
    st.session_state["summary"] = novo_resumo   # atualiza o resumo acumulado no session_state com o novo resumo obtido do modelo, garantindo que o chatbot tenha um contexto atualizado da conversa para gerar respostas mais relevantes e informadas. O resumo acumulado √© uma forma de manter um hist√≥rico condensado da conversa, permitindo que o modelo de linguagem tenha acesso a informa√ß√µes importantes mesmo quando o n√∫mero total de mensagens ultrapassa o limite definido.
    st.session_state["mensagens"] = msgs[-manter_ultimas:] # mant√©m apenas as √∫ltimas manter_ultimas mensagens no hist√≥rico do chat, garantindo que o hist√≥rico seja gerenciado de forma eficiente e que o modelo de linguagem tenha um contexto relevante para gerar respostas, enquanto o resumo acumulado mant√©m as informa√ß√µes importantes das mensagens mais antigas.
#=============================================================
#=========================CHAT=========================
def normalizar_mensagens(): #fun√ß√£o para normalizar o formato das mensagens no session_state, garantindo que todas as mensagens estejam em um formato consistente (dicion√°rios com "role" e "content").
    nova = []  #declara a variavel nova como lista vazia, que serra usada para armazenar as mensagens normalizadas.
    for item in st.session_state["mensagens"]: #para cada item na lista de mensagens do session_state, a fun√ß√£o verifica o formato de cada mensagem e a normaliza para garantir que todas as mensagens estejam em um formato consistente, facilitando o processamento e a gera√ß√£o de respostas pelo modelo de linguagem.
        if isinstance(item, dict) and "role" in item and "content" in item: #se existir um item de dicionario e tiver as chaves "role" e "content".
            if item["role"] in ("user", "assistant", "system"): #se role for user, assistant ou system.
                nova.append(item) #a mensagem j√° est√° no formato correto, ent√£o √© adicionada diretamente √† nova lista (nova[]) de mensagens(item).
        elif isinstance(item, dict) and "text" in item: # se o item √© dicionario com a chave text.
            txt = (item.get("text") or "").strip() #a variavel txt recebe o valor "text" ou vazio e remove os espa√ßos com strip.
            if txt: # se txt n√£o estiver vazio ap√≥s o strip.
                nova.append({"role": "user", "content": txt}) # mensagem √© adicionada a lista.
        elif isinstance(item, str): # se item for string
            txt = item.strip() # a variavel txt recebe o valor do item e remove os espa√ßos com strip.
            if txt: # se txt n√£o estiver vazio ap√≥s o strip.
                nova.append({"role": "user", "content": txt}) #√© adicionado a lista.
    if not nova: # se a lista estiver vazia.
        nova = [{"role": "assistant", "content": "Sextou! Como posso ajudar?"}] #sera adicionado a mensagem inicial
    st.session_state["mensagens"] = nova #atualiza o session_state com a nova lista de mensagens normalizadas, garantindo que o hist√≥rico de mensagens do chat esteja em um formato consistente para processamento e gera√ß√£o de respostas pelo modelo de linguagem.

def ollama_chat(modelo: str, system_prompt: str, contexto: str, pergunta: str) -> str: #cria a fun√ß√£o ollama_chat, que √© respons√°vel por enviar a pergunta do usu√°rio ao modelo de linguagem do Ollama, juntamente com o contexto relevante e o hist√≥rico de mensagens, e retornar a resposta gerada pelo modelo. A fun√ß√£o constr√≥i um payload com as mensagens formatadas, op√ß√µes de gera√ß√£o de texto e faz uma requisi√ß√£o POST para o endpoint de chat do Ollama, processando a resposta para extrair o conte√∫do da mensagem gerada.
    historico = obter_historico_curto(max_msgs=8) #declara a variavel que recebe as 8 mensagens mais recentes do chat.
    mensagens = [   #constr√≥i a lista de mensagens para enviar ao modelo, come√ßando com o system_prompt e o contexto do banco de dados, seguido pelo resumo da conversa at√© agora (se existir) e as mensagens recentes do hist√≥rico, garantindo que o modelo de linguagem tenha acesso a informa√ß√µes relevantes para gerar uma resposta informada e contextualizada.
        {"role": "system", "content": system_prompt},
        {"role": "system", "content": f"DADOS DO BANCO:\n{contexto}"},
    ]
    resumo = (st.session_state.get("summary") or "").strip() #declara a variavel resumo que recebe o resumo acumulado do session_state, ou uma string vazia caso n√£o haja resumo, e remove os espa√ßos em branco no in√≠cio e no final do resumo. O resumo acumulado √© o resultado das chamadas anteriores √† fun√ß√£o de resumo, e √© atualizado a cada vez que o hist√≥rico de mensagens ultrapassa o limite definido, permitindo que o modelo de linguagem mantenha um contexto relevante da conversa ao longo do tempo.
    if resumo: #se resumo n√£o estiver vazio ap√≥s o strip.
        mensagens.append({"role": "system", "content": f"RESUMO DA CONVERSA AT√â AGORA:\n{resumo}"}) #resumo sera adicionado a lista de mensagens como uma mensagem do sistema, fornecendo ao modelo de linguagem um contexto condensado da conversa at√© o momento, o que pode ajudar a gerar respostas mais relevantes e informadas.

    for msg in historico: #para cada mensagem msg no historico(ultimas 8 mensagens).
        if msg.get("role") in ("user", "assistant") and (msg.get("content") or "").strip(): # se mensagem tiver role de user ou assistant e tiver conteudo (content) ou vazio e o conteudo n√£o for vazio ap√≥s o strip.
            if msg["role"] == "user" and msg["content"].strip() == pergunta.strip(): # se for usuario e a mensagem com strip for igual a pergunta com strip. 
                continue #pula esta mensagem para evitar duplicar a pergunta atual no hist√≥rico, garantindo que a pergunta seja adicionada apenas uma vez ao payload enviado para o modelo de linguagem, o que pode ajudar a evitar confus√£o e melhorar a qualidade da resposta gerada.
            mensagens.append({"role": msg["role"], "content": msg["content"]}) #adiciona mensagem.
    
    mensagens.append({"role": "user", "content": pergunta}) #adiciona a pergunta atual do usu√°rio ao final da lista de mensagens, garantindo que o modelo de linguagem tenha acesso √† pergunta mais recente para gerar uma resposta adequada e relevante.
   #assim o payload √© construido com o system_prompt, o contexto do banco de dados a variavel resumo se existir as mensagens recentes so historico e a pergunta atual do usuario, fornecendo ao modelo de linguagem todas as informa√ß√µes necess√°rias para gerar uma resposta informada e contextualizada. 
    payload = {
        "model": modelo, #modelo a ser utilizado para gerar respostas, definido pela vari√°vel modelo passada como argumento da fun√ß√£o.
        "messages": mensagens, #lista de mensagens formatadas que inclui o system_prompt, o contexto do banco de dados, o resumo da conversa (se existir), as mensagens recentes do hist√≥rico e a pergunta atual do usu√°rio, fornecendo ao modelo de linguagem um contexto completo para gerar uma resposta relevante.
        "options": {
            "temperature": 0.15,       # Respostas mais focadas e menos criativas, ideal para recomenda√ß√µes financeiras.
            "top_p": 0.75,             # Considera as palavras mais prov√°veis at√© atingir 75% da distribui√ß√£o, equilibrando qualidade e diversidade.
            "repeat_penalty": 1.18,     # Penaliza repeti√ß√µes, incentivando o modelo a fornecer informa√ß√µes novas e evitar redund√¢ncias.
            "presence_penalty": 0.1,        # Incentiva o modelo a falar sobre novos t√≥picos, mas de forma leve, para manter a relev√¢ncia.
            "frequency_penalty": 0.2        # Penaliza palavras que j√° foram usadas com mais frequ√™ncia, ajudando a evitar respostas repetitivas e incentivando a variedade de vocabul√°rio.
            },
        "stream": False #s√≥ mostra a resposta final sem enviar partes.
    }#O payload √© ent√£o enviado para o endpoint de chat do Ollama, onde o modelo de linguagem processa as mensagens e gera uma resposta com base no contexto e na pergunta fornecidos.
    r = requests.post(OLLAMA_CHAT_URL, json=payload, timeout=REQUEST_TIMEOUT) # variavel r recebe a resposta da requisi√ß√£o POST feita para o endpoint de chat do Ollama, onde o payload √© enviado como JSON e o tempo m√°ximo de espera pela resposta √© definido por REQUEST_TIMEOUT. Esta requisi√ß√£o √© respons√°vel por enviar as mensagens formatadas para o modelo de linguagem e obter a resposta que cont√©m a resposta gerada.
    if r.status_code != 200:# Se deu erro HTTP (ex: modelo n√£o existe).
        raise RuntimeError(f"Ollama HTTP {r.status_code}: {r.text}") #manda mensagem de erro com ostatus code e o texto de resposta.
    data = r.json() #declara a variavel data que recebe o conte√∫do da resposta convertida de JSON para um dicion√°rio Python, permitindo acessar os dados retornados pelo modelo de linguagem, como a mensagem gerada, o resumo atualizado e outras informa√ß√µes relevantes para o chatbot.
    return data["message"]["content"] # retorna o conteudo da mensagem gerada pelo chatbot.
normalizar_mensagens() #chama a fun√ß√£o normalizar_mensagens para garantir que o hist√≥rico de mensagens no session_state esteja em um formato consistente antes de qualquer intera√ß√£o com o modelo de linguagem, facilitando o processamento e a gera√ß√£o de respostas pelo chatbot.
#=============================================================
#==========CARREGAMENTO DE DADOS (MEM√ìRIA CACHE)===============
@st.cache_data(show_spinner=False, ttl=300)  # cache com expira√ß√£o (5 minutos) para os produtos financeiros, que s√£o dados relativamente est√°ticos e n√£o mudam com frequ√™ncia. Isso ajuda a melhorar o desempenho do aplicativo, evitando consultas repetidas ao banco de dados para obter os mesmos dados, enquanto ainda garante que as informa√ß√µes sejam atualizadas periodicamente.
def carregar_produtos() -> pd.DataFrame: #fun√ß√£o para carregar os produtos financeiros do banco de dados e retornar um DataFrame do Pandas.
    # Igual para todos ‚Üí cache global
    return ler_df("SELECT * FROM produtos_financeiros;")


@st.cache_data(show_spinner=False, ttl=300)  # cache com expira√ß√£o (5 minutos) para os dados do usu√°rio, que podem mudar com o tempo (por exemplo, perfil de investidor, metas, hist√≥rico
def carregar_dados_usuario(cpf_digits: str) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]: #fun√ß√£o para carregar os dados do usu√°rio (perfil, metas, hist√≥rico, transa√ß√µes) do banco de dados com base no CPF fornecido, retornando uma tupla de DataFrames do Pandas.
    perfil = ler_df( #query SQL para obter o perfil do investidor com base no CPF fornecido.
        """
        SELECT nome_investidor, cpf, idade, profissao, renda_mensal, perfil,
               objetivo_principal, patrimonio, reserva, risco
        FROM perfil_investidor
        WHERE regexp_replace(cpf::text, '[^0-9]', '', 'g') = %s
        LIMIT 1;
        """,
        params=(cpf_digits,) #par√¢metros para a consulta SQL, onde cpf_digits √© o CPF do usu√°rio sem formata√ß√£o (apenas d√≠gitos
    )
    metas = ler_df( #query SQL para obter as metas do investidor com base no CPF fornecido.
        """
        SELECT * FROM metas_investidor
        WHERE regexp_replace(cpf_investidor::text, '[^0-9]', '', 'g') = %s;
        """,
        params=(cpf_digits,)    
    )
    historico = ler_df(  #query SQL para obter o hist√≥rico de atendimento do investidor com base no CPF fornecido.
        """
        SELECT * FROM historico_atendimento
        WHERE regexp_replace(cpf::text, '[^0-9]', '', 'g') = %s
        ORDER BY data DESC
        LIMIT 10;
        """,
        params=(cpf_digits,)
    ) #adicionei um limite de 10 para demonstra√ß√£o pois se fosse em um caso real poderia gerar uma sobrecarga de informa√ß√µes.
    transacoes = ler_df( #query SQL para obter as transa√ß√µes do investidor com base no CPF fornecido.
        """
        SELECT * FROM transacoes
        WHERE regexp_replace(cpf::text, '[^0-9]', '', 'g') = %s
        ORDER BY data DESC
        LIMIT 20;
        """,
        params=(cpf_digits,)
    )#adicionei um limite de 20 para demonstra√ß√£o pois se fosse em um caso real poderia gerar uma sobrecarga de informa√ß√µes.
    return perfil, metas, historico, transacoes #retorna os DataFrames do perfil, metas, hist√≥rico e transa√ß√µes do usu√°rio.

@st.cache_data(show_spinner=False, ttl=300)  # cache com expira√ß√£o (5 minutos) em bora os dados s√£o fixos nesta aplica√ß√£o fica aqui uma melhoria pro mundo real que √© para n√£o ficar com dados muito antigos caso o usu√°rio deixe o app aberto por muito tempo
def montar_contexto(cpf_digits: str | None) -> tuple[str, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]: #fun√ß√£o para montar o contexto completo para o chatbot, incluindo os produtos financeiros e os dados do usu√°rio (perfil, metas, hist√≥rico, transa√ß√µes) se o CPF for fornecido. A fun√ß√£o retorna uma tupla contendo o contexto em formato de texto e os DataFrames correspondentes.
    """
    Retorna:
      contexto_texto, produtos, perfil, metas, historico, transacoes
    """
    partes = [] #declara a variavel partes como uma lista vazia
    produtos = carregar_produtos() # variavel produtos recebe o resultado da fun√ß√£o carregar_produtos, que √© um DataFrame do Pandas contendo os produtos financeiros dispon√≠veis no banco de dados.
    partes.append("### produtos_financeiros (COMPLETO)\n" + produtos.to_csv(index=False) + "\n") # adiciona o conteudos a partes.

    if not cpf_digits: # se n√£o tem cpf usuario esta como visitante, ent√£o n√£o tem dados para filtrar, entao √© adicionado um aviso a partes e as variaveis de perfil, metas, historico e transacoes recebem DataFrames vazios, garantindo que o chatbot funcione mesmo para usu√°rios sem identifica√ß√£o, mas com acesso limitado a recomenda√ß√µes gen√©ricas.
        partes.append("### AVISO\nUsu√°rio sem identifica√ß√£o: n√£o h√° dados pessoais para filtrar.\n")
        vazio = pd.DataFrame()  #variavel vazio recebe o DataFrame vazio do Pandas.
        return "\n".join(partes), produtos, vazio, vazio, vazio, vazio #retorna o contexto em formato de texto, os produtos financeiros e DataFrames vazios para perfil, metas, hist√≥rico e transa√ß√µes.

    perfil, metas, historico, transacoes = carregar_dados_usuario(cpf_digits) #se o CPF for fornecido, as vari√°veis perfil, metas, historico e transacoes recebem os DataFrames correspondentes aos dados do usu√°rio, que s√£o carregados usando a fun√ß√£o carregar_dados_usuario com base no CPF fornecido. Esses dados ser√£o usados para personalizar as recomenda√ß√µes de investimento e fornecer um contexto mais relevante para o chatbot.

    partes.append("### perfil_investidor (CPF logado)\n" + perfil.to_csv(index=False) + "\n")       #adiciona o conteudo do perfil do investidor.
    partes.append("### metas_investidor (CPF logado)\n" + metas.to_csv(index=False) + "\n")     #adiciona o conteudo das metas do investidor.
    partes.append("### historico_atendimento (CPF logado)\n" + historico.to_csv(index=False) + "\n")    #adiciona o conteudo do historico de atendimento do investidor.
    partes.append("### transacoes (CPF logado)\n" + transacoes.to_csv(index=False) + "\n")      #adiciona o conteudo das transa√ß√µes do investidor.
    #retorna o contexto completo em formato de texto, os produtos financeiros e os DataFrames do perfil, metas, hist√≥rico e transa√ß√µes do usu√°rio, que ser√£o usados para personalizar as recomenda√ß√µes de investimento e fornecer um contexto mais relevante para o chatbot.
    return "\n".join(partes), produtos, perfil, metas, historico, transacoes  

if st.session_state.get("logado"): #se o usu√°rio estiver logado, a vari√°vel cpf_digits recebe o CPF do usu√°rio formatado, convertido para apenas d√≠gitos usando a fun√ß√£o cpf_para_digitos. Isso √© necess√°rio para filtrar os dados do usu√°rio no banco de dados e fornecer um contexto personalizado para o chatbot, permitindo que ele fa√ßa recomenda√ß√µes de investimento com base no perfil,
    cpf_digits = cpf_para_digitos(st.session_state.cpf_formatado) #a fun√ß√£o cpf_para_digitos √© usada para remover qualquer formata√ß√£o do CPF (como pontos e tra√ßos) e retornar apenas os d√≠gitos, garantindo que o CPF seja tratado de forma consistente ao consultar o banco de dados e montar o contexto para o chatbot.
else:
    cpf_digits = None #se o usu√°rio n√£o estiver logado, a vari√°vel cpf_digits √© definida como None, indicando que n√£o h√° um CPF espec√≠fico para filtrar os dados do usu√°rio no banco de dados. Isso significa que o chatbot funcionar√° em um modo mais gen√©rico, sem acesso a informa√ß√µes personalizadas do usu√°rio, e as recomenda√ß√µes de investimento ser√£o baseadas apenas nos produtos financeiros dispon√≠veis, sem considerar o perfil ou hist√≥rico do usu√°rio.

contexto_dados, produtos, perfil, metas, historico, transacoes = montar_contexto(cpf_digits) #chama a fun√ß√£o montar_contexto com o cpf_digits (que pode ser None para usu√°rios n√£o logados) para obter o contexto completo em formato de texto, os produtos financeiros e os DataFrames do perfil, metas, hist√≥rico e transa√ß√µes do usu√°rio. Esses dados ser√£o usados para personalizar as recomenda√ß√µes de investimento e fornecer um contexto mais relevante para o chatbot ao gerar respostas para as perguntas dos usu√°rios.
#=============================================================
#=========================FUN√á√ïES DE √ÅUDIO (TTS)=========================
language = "pt-BR"  #define idioma para portugu√™s, utilizado em fun√ß√µes de transcri√ß√£o e TTS para garantir que o reconhecimento de voz e a gera√ß√£o de √°udio sejam adequados para o p√∫blico-alvo do chatbot, que √© falante de portugu√™s.
def transcrever_audio_google(audio_bytes: bytes, language: str = "pt-BR") -> str: #Fun√ß√£o Transcreve √°udio usando SpeechRecognition + Google Web Speech (precisa de internet).
    r = sr.Recognizer()  # declara a variavel r q recebe a instancia de reconhecer a voz da bibliteca SpeechRecognition.
    with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as f: # Cria arquivo temporario (Named) para salvar audio o delete √© falso para poder usar o arquivo depois do with e o suffix √© .wav porque o audio recebido do streamlit √© em formato wav.
        f.write(audio_bytes) #escreve os bytes do √°udio recebido para o arquivo tempor√°rio criado, permitindo que a biblioteca SpeechRecognition acesse o √°udio para transcri√ß√£o. O arquivo tempor√°rio √© necess√°rio porque a fun√ß√£o AudioFile do SpeechRecognition espera um caminho de arquivo, e n√£o pode processar diretamente os bytes do √°udio.
        caminho = f.name #declara a variavel caminho que recebe o nome do arquivo tempor√°rio criado, que √© o caminho para o arquivo de √°udio que ser√° processado pela biblioteca Speech

    try: #tente
        with sr.AudioFile(caminho) as source: #com o arquivo de audio variavel caminho aberto 
            audio_data = r.record(source) # a variavel audio_data recebe o conte√∫do do arquivo de √°udio processado pela biblioteca SpeechRecognition, que √© usado para transcri√ß√£o. A fun√ß√£o record() √© usada para ler o √°udio do arquivo e armazen√°-lo em um formato que pode ser enviado para o servi√ßo de reconhecimento de voz do Google para obter a transcri√ß√£o em texto.
        texto = r.recognize_google(audio_data, language=language) # a variavel texto recebe a transcri√ß√£o do √°udio processado, usando o servi√ßo de reconhecimento de voz do Google. A fun√ß√£o recognize_google() √© chamada com os dados de √°udio e o idioma especificado para obter a transcri√ß√£o em texto do √°udio, que pode ser usada como entrada para o chatbot ou para outras funcionalidades do aplicativo.
        return texto.strip()  #retorna o texto transcrito, removendo espa√ßos em branco no inicio e final da string
    finally: #finalmente
        try: #tente
            os.remove(caminho) #remover o caminho garantindo que o arquivo tempor√°rio seja excluido do sistema evitando acumulo de arquivos.
        except Exception: #se der erro s√≥ passa pois isso n√£o √© critico e evita do app quebrar por por um arquivo temporario.
            pass 
def texto_para_audio_mp3_bytes(texto: str, language: str = "pt") -> bytes: #Gera MP3 (bytes) com gTTS (precisa de internet).
    tts = gTTS(text=texto, lang=language) #tts recebe a instancia do gtt com o texto e idioma definido para gerar audio.
    buf = io.BytesIO()  # buf √© armazenado io.BytesIO() ou seja ele vai armazenar os bytes do audio gerado na mem√≥ria.
    tts.write_to_fp(buf) # tts escreve os bytes do audio armazenados em buf.
    return buf.getvalue() # retorna os bytes de audio gerados.
def limpar_para_tts(texto: str) -> str:  #limpa o texto para melhorar pronuncia em TTS removendo markdown,c√≥digos e simbolos.
    RE_CODEBLOCK = re.compile(r"```.*?```", re.S) 
    RE_MD = re.compile(r"[*_`|>#]")
    RE_TECH = re.compile(r"[\\/+=~^]")
    RE_SPACES = re.compile(r"\s+")

    texto = RE_CODEBLOCK.sub(" ", texto) 
    texto = RE_MD.sub(" ", texto)
    texto = RE_TECH.sub(" ", texto)
    texto = RE_SPACES.sub(" ", texto).strip()
    return texto #retorna texto limpo.
#===============================================================
#=========================MENU LATERAL=========================
with st.sidebar: #declara que tudo aqui √© do menu lateral do Streamlit.
    st.header("‚öôÔ∏è Configura√ß√µes") #t√≠tulo do menu lateral.

    try:  #tente
        resp = requests.get(f"{Ollama_BASE}/api/tags", timeout=20) #resp recebe a requisi√ßa√µ GET para o end point de tags do Ollama para verificar se o servi√ßo est√° online, com um tempo m√°ximo de espera de 20 segundos. Esta requisi√ß√£o √© usada para determinar o status do servi√ßo do Ollama.
        cor = resp.status_code # cor recebe o status code de resp.
    except Exception as e: #exceto se der erro na requisi√ß√£o 
        st.write("Erro de conex√£o:", e) #escreve mensagem de erro exibindo a exce√ß√£o que ocorreu durante tentativa de conex√£o.
        cor = 500 # cor recebe valor de 500 
    if cor == 200: # cor deu 200 ent√£o aparece os status text e cor verde, caso contr√°rio vermelho.
        status_text = "üü¢ Online"
        status_color = "#008000"
    else:
        status_text = "üî¥ Offline"
        status_color = "#FF0000"

    st.markdown( #escreve o status do servi√ßo do Ollama no menu lateral, usando markdown para formatar o texto com a cor definida por status_color e deixando o texto em negrito.
        f"<span style='color:{status_color}; font-weight:bold;'>Status: {status_text}</span>",
        unsafe_allow_html=True
    )
    modelo = st.text_input("Modelo (Ollama)", value=DEFAULT_MODEL) # campo de texto onde usu√°rio ve o modelo que esta sendo utilizado.

    st.divider() #linha divis√≥ria para separar visualmente as se√ß√µes do menu lateral.
    st.subheader("Usu√°rio")  #subt√≠tulo da se√ß√£o de usu√°rio no menu lateral.

    if st.session_state.logado: # se o usuario estive logado.
        st.sidebar.success("‚úÖ Logado") #exibe mensagem logado.
        st.sidebar.write(f"CPF: {st.session_state.cpf_formatado}") #exibe o cpf do usu√°rio formatado na lateral.
        carrega_no_ollama = cpf_para_digitos(st.session_state.cpf_formatado) #variavel carrega_no_ollama recebe o cpf do usuario formatado.
    
    elif st.session_state.visitante: #se o usuario for visitante.
        st.sidebar.info("üë§ Visitante")  #mosta mensagem de visitante.
        carrega_no_ollama = None #variavel carrega_no_ollama recebe None.

    if st.sidebar.button("Sair"): #se clicar em sair
        st.session_state.logado = False # recebe False, ou seja, o usu√°rio n√£o est√° mais logado voltando para pagina de acesso.
        st.session_state.visitante = False # recebe False, ou seja, o usu√°rio n√£o est√° mais logado voltando para pagina de acesso.
        st.session_state.cpf_formatado = "" #limpa o cpf formatado do session_state.
        st.session_state.mensagens = [{"role": "assistant", "content": "Sextou! Como posso ajudar?"}] #reseta as mensagens para a mensagem inicial.
        carrega_no_ollama = None #variavel carrega_no_ollama recebe None.
        st.rerun() #recarrega a p√°gina para refletir as mudan√ßas no estado, como o logout do usu√°rio e a limpeza do hist√≥rico de mensagens, garantindo que o chatbot volte ao estado inicial e esteja pronto para um novo usu√°rio ou para o mesmo usu√°rio fazer login novamente.
    
    st.divider() #linha divis√≥ria para separar visualmente as se√ß√µes do menu lateral.
    st.subheader("üîä Voz")# subtitulo de se√ß√£o

    permitir_audio = st.toggle("üéôÔ∏è Enviar √°udio (microfone)", value=True) # bot√£o para usuario configurar qeur utilizar microfone, inicialmente configurado com true.
    
    tts_on = st.toggle("üîä Bot responder em √°udio (TTS)", value=True) # bot√£o para usuario configurar quer que IA fale, inicialmento configurado como true.
    
    st.divider() #linha divis√≥ria para separar visualmente as se√ß√µes do menu lateral.
    st.subheader("üßπ Limpar hist√≥rico") #subt√≠tulo da se√ß√£o de limpeza de hist√≥rico no menu lateral.
    limpar = st.button("üßπ Limpar chat") #bot√£o limpar
    if limpar: # se clicar em limpar chat
        st.session_state.mensagens = []  #limpa as mensagens do session_state, removendo todo o hist√≥rico de mensagens do chat.     
        st.session_state.mensagens = [{"role": "assistant", "content": "Conversa limpa. Sextou! Como posso ajudar?"}]   #adiciona uma mensagem inicial ao hist√≥rico.
        st.session_state.summary = "" #limpa o resumo acumulado do session_state, garantindo que o chatbot comece com um contexto limpo e sem informa√ß√µes anteriores.
        st.rerun() #recarrega a pagina para refletir as mudan√ßas no estado.
    st.divider() #linha divis√≥ria para separar visualmente as se√ß√µes do menu lateral.
    st.subheader("üíæ Exportar hist√≥rico") 
    mensagens_txt = "\n".join([f"{m['role'].upper()}: {m['content']}" for m in st.session_state.mensagens]) 
    #variavel mensagens_txt recebe uma string formatada que cont√©m o hist√≥rico de mensagens do chat, onde cada mensagem √© representada por seu papel (role) em letras mai√∫sculas seguido pelo conte√∫do da mensagem. O m√©todo join() √© usado para concatenar todas as mensagens em uma √∫nica string, separando cada mensagem por uma nova linha (\n), facilitando a leitura e exporta√ß√£o do hist√≥rico de mensagens.
    resumo = st.session_state.get("summary", "") #variavel resumo recebe o resumo acumulado do session_state, ou uma string vazia caso n√£o haja resumo. O resumo acumulado √© o resultado das chamadas anteriores √† fun√ß√£o de resumo, e √© atualizado a cada vez que o hist√≥rico de mensagens ultrapassa o limite definido, permitindo que o modelo de linguagem mantenha um contexto relevante da conversa ao longo do tempo. Se houver um resumo dispon√≠vel.
    if resumo: #seresumo n√£o estiver vazio.
        mensagens_txt = f"RESUMO:\n{resumo}\n\n{mensagens_txt}" #√© adicionado ao inicio de mensagens_txt.
    st.download_button("Baixar hist√≥rico (.txt)", mensagens_txt, file_name="historico_chat.txt") #bot√£o para baixar o historico de mensagens.
#==============================================================
#=========================CORPO DO APP=========================
st.title("Sexta-Feira - Assistente de Investimentos üöÄ") # t√≠tulo do app 

for m in st.session_state["mensagens"]: #para cada mensagem m no historico de mensagens.
    with st.chat_message(m["role"], avatar=("üßë‚Äçüíª" if m["role"] == "user" else "üåê")): #com se o role for de user ent√£o √© um avatar se n√£o √© o outro.
        st.write(m["content"]) #escreve o conte√∫do da mensagem no chat, garantindo que o hist√≥rico de mensagens seja exibido corretamente com os avatares correspondentes para o usu√°rio e o assistente.

entrada = st.chat_input("Digite sua mensagem aqui...", accept_audio=permitir_audio) #campo de entrada para o usu√°rio digitar sua mensagem, com um placeholder "Digite sua mensagem aqui..." e a op√ß√£o de aceitar √°udio configurada com base na vari√°vel permitir_audio, permitindo que o usuario tire essa op√ß√£o pelo menu lateral.

texto_usuario = "" # declara a variavel como string vazia.
audio_bytes = None #declara variavel como none
audio_file = None #declara variavel como none

if entrada is not None: #se o usu√°rio enviou uma entrada (texto ou √°udio).

    if isinstance(entrada, str): #se entrada for do tipo string
        texto_usuario = entrada.strip() #a variavel texto usuario recebe este valor

    elif hasattr(entrada, "text"): #se n√£o se for identificao como text.
        texto_usuario = (entrada.text or "").strip()    # texto recebe entrada do tipo texto ou fica vazia
        audio_file = getattr(entrada, "audio", None)    # tenta pegar audio se n√£o consegue fica none

    else:
        texto_usuario = str(entrada).strip() #se n√£o texto usuario √© a entrada convetida em string

    if audio_file is not None: #se arquivo de audio n√£o esta vazio
        audio_bytes = audio_file.getvalue()  #variavel recebe valor do arquivo

    if (not texto_usuario) and audio_bytes: # Se n√£o digitou texto, mas veio √°udio: transcreve
        try: #tente
            texto_usuario = transcrever_audio_google(audio_bytes, language) #chama fun√ß√£o com os parametros audio_bytes e language
            # st.toast(f"üìù {texto_usuario}") c√≥digo utilizado mais para teste porem resolvi deixar porque achei bonito e dependendo da opni√£o de cada um seeria mais legal deixar ou tirar essa funcionalidade
        except Exception as e:  #excess√£o de erro
            st.error("Erro ao transcrever √°udio.")
            st.exception(e)
            texto_usuario = ""

    if texto_usuario: #S√≥ processa se tiver texto
        st.session_state["mensagens"].append({"role": "user", "content": texto_usuario}) # salva msg do usu√°rio
        with st.chat_message("user", avatar="üßë‚Äçüíª"): #adciona avatar de usuario
            st.write(texto_usuario) #escreve o texto no chat

        with st.chat_message("assistant", avatar="üåê"):# chama IA definindo um avatar IA
            thinking_placeholder = st.empty() #adiciona um icone de pensamento com a menssagem enquanto que IA elabora resposta
            thinking_placeholder.info("Sexta-Feira est√° pensando...")
            try: #tenta resposta pela fun√ß√£o ollama chat chamando o texto do usuario, os dados do banco de dados que est√£o em memoria cach sua mem√≥ria e seu system prompt.
                resposta = ollama_chat(modelo, SYSTEM_PROMPT_DEFAULT, contexto_dados, texto_usuario)
            except Exception as e: #caso de erro mostrar as seguntem mensagens
                st.error("Falha ao chamar o Ollama (erro real abaixo):")
                st.exception(e)  # <- mostra stack trace completo no Streamlit
                resposta = "N√£o consegui acessar o Ollama. Veja o erro acima."

            thinking_placeholder.empty() #icone de pensamento some
            st.write(resposta) #escreve a resposta

        st.session_state["mensagens"].append({"role": "assistant", "content": resposta}) # mostra resposta no chat.
        try: #chama fun√ß√£o para averiguar se ja pode criar um resumo pra IA.
            talvez_resumir_chat(modelo)
        except Exception as e: #caso de erro mostrar mensagem.
            st.warning("N√£o consegui resumir o hist√≥rico agora.")
            st.caption(str(e))

        if tts_on: #se o bot√£o do menu lateral tts tiver true.
            try: #chamar fun√ß√£o  para converter texto em audio.
                texto_limpo = limpar_para_tts(resposta)
                mp3_bytes = texto_para_audio_mp3_bytes(texto_limpo, language)
                st.audio(mp3_bytes, format="audio/mp3")
            except Exception as e: #caso de erro mostrar mensagem.
                st.warning("N√£o consegui gerar o √°udio da resposta.")
                st.caption(str(e))

        MAX_MENSAGENS = 20 
        if len(st.session_state["mensagens"]) > MAX_MENSAGENS:  ##numero maximo a ser exibida na tela de mensagem no chatbot
            st.session_state["mensagens"] = st.session_state["mensagens"][-MAX_MENSAGENS:]

#=============================================================
#=========================FIM DO APP=========================    